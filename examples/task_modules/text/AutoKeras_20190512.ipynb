{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "### AutoKeras\n",
    "\n",
    "CodeFest, May 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bmcmahon/.pytorch_pretrained_bert\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import urllib\n",
    "import zipfile\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from autokeras.utils import read_tsv_file\n",
    "from autokeras.text.text_supervised import TextClassifier\n",
    "\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url_to_filepath(outpath, url):\n",
    "    \"\"\"Create path and download data from url.\"\"\"\n",
    "    fd, fn = re.findall(r\"^(.+\\/)([^\\/]+)$\",outpath)[0]\n",
    "    fp = fd + fn\n",
    "    if not os.path.exists(fd):\n",
    "        os.makedirs(fd)\n",
    "    if not os.path.exists(fp):\n",
    "        urllib.request.urlretrieve(url, fp) \n",
    "    return fp \n",
    "\n",
    "def unzip_file(in_path,out_dir):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(fd)\n",
    "        if in_path[-3:]==\"zip\":\n",
    "            z = zipfile.ZipFile(in_path,'r')\n",
    "            z.extractall(out_dir)\n",
    "            z.close()\n",
    "        elif in_path[-6:]==\"tar.gz\":\n",
    "            tar = tarfile.open(fp)\n",
    "            tar.extractall(path=out_dir)\n",
    "            tar.close()    \n",
    "    return out_dir\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Loads Keras dataset preprocessed into integers by frequency of occurrence, 1 being most frequent\n",
    "    \n",
    "    Args\n",
    "    :path: path/to/keras/dataset\n",
    "    Returns two tuples of train and test\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path=path)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def convert_labels_to_one_hot(labels, num_labels):\n",
    "    one_hot = np.zeros((len(labels), num_labels))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "def convert_int_to_word(integer):\n",
    "    return vocab.values[integer][0]\n",
    "\n",
    "def convert_int_to_str_array(array):\n",
    "    return np.array([\" \".join(map(str,[convert_int_to_word(n) for n in integer])) for integer in array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Vocab from [Stanford](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "Preprocessed data from [Keras](https://keras.io/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = download_url_to_filepath(\"/tmp/imdb.tar.gz\",\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
    "fd = unzip_file(fp,\"/tmp/imdb\")\n",
    "vocab = pd.read_csv(os.path.join(fd,\"aclImdb/imdb.vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.shape)\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(fd,\"aclImdb/train/neg/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(fd,\"aclImdb/train/neg/1821_4.txt\")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_object = open(os.path.join(fd,\"aclImdb/train/neg/1821_4.txt\"),\"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataframe(in_dir,label):\n",
    "    lst = []\n",
    "    for fil in os.listdir(in_dir):\n",
    "        lst.append(open(os.path.join(in_dir,fil),'r').readlines()[0])\n",
    "    df = pd.DataFrame(lst,columns=[\"review\"])\n",
    "    df['sentiment']=label\n",
    "    return df\n",
    "\n",
    "def prep_sample(in_dir, sample_type):\n",
    "    neg = prep_dataframe(os.path.join(in_dir,f\"aclImdb/{sample_type}/neg/\"),0)\n",
    "    pos = prep_dataframe(os.path.join(in_dir,f\"aclImdb/{sample_type}/pos/\"),1)\n",
    "    df = pd.concat([neg,pos])\n",
    "    df = df.sample(frac=1)\n",
    "    X = np.array(df['review'])\n",
    "    Y = np.array(df['sentiment'])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = prep_sample(fd,'train')\n",
    "x_test, y_test = prep_sample(fd,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg = prep_dataframe(os.path.join(fd,\"aclImdb/train/neg/\"),0)\n",
    "# pos = prep_dataframe(os.path.join(fd,\"aclImdb/train/pos/\"),1)\n",
    "# df = pd.concat([neg,pos])\n",
    "# df = df.sample(frac=1)\n",
    "# x_train = np.array(df['review'])\n",
    "# y_train = np.array(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed Keras Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = load_data(path=\"imdb.npz\")\n",
    "print(f\"X,Y Train: {len(x_train),len(x_train[0])},{len(y_train)}\")\n",
    "print(f\"X,Y Test: {len(x_test),len(x_test[0])},{len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = (x_train, y_train)\n",
    "# x_test, y_test = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = convert_int_to_str_array(x_train)\n",
    "# x_test = convert_int_to_str_array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = convert_labels_to_one_hot(y_train, num_labels=2)\n",
    "y_test = convert_labels_to_one_hot(y_test, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(verbose=True)\n",
    "clf.fit(x=x_train, y=y_train, time_limit=12 * 60 * 60)\n",
    "print(\"Classification accuracy is : \", 100 * clf.evaluate(x_test, y_test), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_dir(bucket, dir_key, file_list, out_dir):\n",
    "    \"\"\"Downloads a file directory from an s3 bucket path.\n",
    "    Args\n",
    "    :bucket: str s3 bucket\n",
    "    :dir_key: str path from bucket to file, exclusive\n",
    "    :file_list: list files in directory to download\n",
    "    :out_dir: path/to/output\n",
    "    \"\"\"\n",
    "    if os.path.exists(out_dir):\n",
    "        return\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    for fil in file_list:\n",
    "        print(f\"Downloading {fil}\")\n",
    "        key = dir_key + fil\n",
    "        fp = out_dir + fil\n",
    "        s3.Bucket(bucket).download_file(key, fp)\n",
    "    print(f\"Files saved to {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"mbbu.pth\",\"vbbu.txt\"]\n",
    "download_s3_dir(\"nucleus-chc-preprod-datasciences\",\n",
    "                \"users/bmcmahon/nas/pytorch_pretrained_bert/\",\n",
    "                file_list,\n",
    "                os.path.join(\"/tmp\",\n",
    "                             \".pytorch_pretrained_bert/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# op = os.path.join(os.path.expanduser(\"~\"),\".pytorch_pretrained_bert/\")\n",
    "# subprocess.run([\"aws\",\"s3\",\"cp\",\"s3://nucleus-chc-preprod-datasciences/users/bmcmahon/nas/pytorch_pretrained_bert\",op,\"--recursive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_s3_path(\"s3://nucleus-chc-preprod-datasciences/users/bmcmahon/nas/pytorch_pretrained_bert\",\n",
    "#                  os.path.join(os.path.expanduser(\"~\"),\".pytorch_pretrained_bert/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(os.path.expanduser(\"~\"),\".pytorch_pretrained_bert/\")):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory(bucket_name, directory_path, download_path, exclude_file_names):\n",
    "    # prepare session\n",
    "    session = Session(aws_access_key_id, aws_secret_access_key, region_name)\n",
    "\n",
    "    # get instances for resource and bucket\n",
    "    resource = session.resource('s3')\n",
    "    bucket = resource.Bucket(bucket_name)\n",
    "\n",
    "    for s3_key in self.client.list_objects(Bucket=bucket_name, Prefix=directory_path)['Contents']:\n",
    "        s3_object = s3_key['Key']\n",
    "        if s3_object not in exclude_file_names:\n",
    "            bucket.download_file(file_path, download_path + str(s3_object.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "#initiate s3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# select bucket\n",
    "my_bucket = s3.Bucket('nucleus-chc-prepod-datasciences')\n",
    "\n",
    "# download file into current directory\n",
    "for s3_object in my_bucket.objects.all():\n",
    "    # Need to split s3_object.key into path and file name, else it will give error file not found.\n",
    "    path, filename = os.path.split(s3_object.key)\n",
    "    my_bucket.download_file(s3_object.key, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoKeras",
   "language": "python",
   "name": "autokeras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
